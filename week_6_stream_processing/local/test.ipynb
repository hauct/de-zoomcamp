{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from decimal import Decimal\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class Ride:\n",
    "    def __init__(self, arr: List[str]):\n",
    "        self.vendor_id = int(arr[0])\n",
    "        self.tpep_pickup_datetime = datetime.strptime(arr[1], \"%Y-%m-%d %H:%M:%S\")\n",
    "        self.tpep_dropoff_datetime = datetime.strptime(arr[2], \"%Y-%m-%d %H:%M:%S\")\n",
    "        self.passenger_count = int(arr[3])\n",
    "        self.trip_distance = Decimal(arr[4])\n",
    "        self.rate_code_id = int(arr[5])\n",
    "        self.store_and_fwd_flag = arr[6]\n",
    "        self.pu_location_id = int(arr[7])\n",
    "        self.do_location_id = int(arr[8])\n",
    "        self.payment_type = arr[9]\n",
    "        self.fare_amount = Decimal(arr[10])\n",
    "        self.extra = Decimal(arr[11])\n",
    "        self.mta_tax = Decimal(arr[12])\n",
    "        self.tip_amount = Decimal(arr[13])\n",
    "        self.tolls_amount = Decimal(arr[14])\n",
    "        self.improvement_surcharge = Decimal(arr[15])\n",
    "        self.total_amount = Decimal(arr[16])\n",
    "        self.congestion_surcharge = Decimal(arr[17])\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, d: Dict):\n",
    "        return cls(arr=[\n",
    "            d['vendor_id'],\n",
    "            d['tpep_pickup_datetime'],\n",
    "            d['tpep_dropoff_datetime'],\n",
    "            d['passenger_count'],\n",
    "            d['trip_distance'],\n",
    "            d['rate_code_id'],\n",
    "            d['store_and_fwd_flag'],\n",
    "            d['pu_location_id'],\n",
    "            d['do_location_id'],\n",
    "            d['payment_type'],\n",
    "            d['fare_amount'],\n",
    "            d['extra'],\n",
    "            d['mta_tax'],\n",
    "            d['tip_amount'],\n",
    "            d['tolls_amount'],\n",
    "            d['improvement_surcharge'],\n",
    "            d['total_amount'],\n",
    "            d['congestion_surcharge'],\n",
    "        ]\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}: {self.__dict__}'\n",
    "\n",
    "\n",
    "def ride_to_dict(ride: Ride, ctx):\n",
    "    return ride.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "2020-07-01 00:25:32\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'vendor_id': '123',\n",
    "    'tpep_pickup_datetime': '2020-07-01 00:25:32',\n",
    "    'tpep_dropoff_datetime': '2020-07-01 00:33:39',\n",
    "    'passenger_count': '2',\n",
    "    'trip_distance': '5.0',\n",
    "    'rate_code_id': '1',\n",
    "    'store_and_fwd_flag': 'N',\n",
    "    'pu_location_id': '101',\n",
    "    'do_location_id': '102',\n",
    "    'payment_type': 'Credit Card',\n",
    "    'fare_amount': '15.00',\n",
    "    'extra': '1.50',\n",
    "    'mta_tax': '0.50',\n",
    "    'tip_amount': '2.00',\n",
    "    'tolls_amount': '0.00',\n",
    "    'improvement_surcharge': '0.30',\n",
    "    'total_amount': '19.30',\n",
    "    'congestion_surcharge': '2.50',\n",
    "}\n",
    "\n",
    "# Sử dụng phương thức from_dict để tạo một đối tượng Ride từ từ điển data\n",
    "ride = Ride.from_dict(data)\n",
    "\n",
    "# Đối tượng ride bây giờ chứa thông tin từ từ điển data\n",
    "print(ride.vendor_id)  # Kết quả: 123\n",
    "print(ride.tpep_pickup_datetime)  # Kết quả: 2023-09-25 12:00:00\n",
    "# ...và cùng các thuộc tính khác"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ride: {'vendor_id': 1, 'tpep_pickup_datetime': datetime.datetime(2020, 7, 1, 0, 25, 32), 'tpep_dropoff_datetime': datetime.datetime(2020, 7, 1, 0, 33, 39), 'passenger_count': 1, 'trip_distance': Decimal('1.50'), 'rate_code_id': 1, 'store_and_fwd_flag': 'N', 'pu_location_id': 238, 'do_location_id': 75, 'payment_type': '2', 'fare_amount': Decimal('8'), 'extra': Decimal('0.5'), 'mta_tax': Decimal('0.5'), 'tip_amount': Decimal('0'), 'tolls_amount': Decimal('0'), 'improvement_surcharge': Decimal('0.3'), 'total_amount': Decimal('9.3'), 'congestion_surcharge': Decimal('0')}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from typing import List\n",
    "# from ride import Ride\n",
    "\n",
    "import os\n",
    "\n",
    "CURRENT_FILE_PATH = os.getcwd()\n",
    "INPUT_DATA_PATH = os.path.join(CURRENT_FILE_PATH, 'resources', 'data', 'rides.csv')\n",
    "\n",
    "\n",
    "def read_rides(resource_path: str = INPUT_DATA_PATH) -> List[Ride]:\n",
    "    rides = []\n",
    "    with open(resource_path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)  # skip the header row\n",
    "        for row in reader:\n",
    "            rides.append(Ride(arr=row))\n",
    "    return rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides = []\n",
    "with open(INPUT_DATA_PATH, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)  # skip the header row\n",
    "    for row in reader:\n",
    "        row_0 = row\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '2020-07-01 00:25:32',\n",
       " '2020-07-01 00:33:39',\n",
       " '1',\n",
       " '1.50',\n",
       " '1',\n",
       " 'N',\n",
       " '238',\n",
       " '75',\n",
       " '2',\n",
       " '8',\n",
       " '0.5',\n",
       " '0.5',\n",
       " '0',\n",
       " '0',\n",
       " '0.3',\n",
       " '9.3',\n",
       " '0']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ride: {'vendor_id': 1, 'tpep_pickup_datetime': datetime.datetime(2020, 7, 1, 0, 25, 32), 'tpep_dropoff_datetime': datetime.datetime(2020, 7, 1, 0, 33, 39), 'passenger_count': 1, 'trip_distance': Decimal('1.50'), 'rate_code_id': 1, 'store_and_fwd_flag': 'N', 'pu_location_id': 238, 'do_location_id': 75, 'payment_type': '2', 'fare_amount': Decimal('8'), 'extra': Decimal('0.5'), 'mta_tax': Decimal('0.5'), 'tip_amount': Decimal('0'), 'tolls_amount': Decimal('0'), 'improvement_surcharge': Decimal('0.3'), 'total_amount': Decimal('9.3'), 'congestion_surcharge': Decimal('0')}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ride = Ride(row_0)\n",
    "ride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T\n",
    "\n",
    "PRODUCE_TOPIC_RIDES_CSV = CONSUME_TOPIC_RIDES_CSV = 'rides_csv'\n",
    "BOOTSTRAP_SERVERS = 'localhost:9092'\n",
    "\n",
    "TOPIC_WINDOWED_VENDOR_ID_COUNT = 'vendor_counts_windowed'\n",
    "\n",
    "RIDE_SCHEMA1 = T.StructType(\n",
    "    [T.StructField(\"vendor_id\", T.IntegerType()),\n",
    "     T.StructField('tpep_pickup_datetime', T.StringType()),\n",
    "     T.StructField('tpep_dropoff_datetime', T.StringType()),\n",
    "     T.StructField(\"passenger_count\", T.IntegerType()),\n",
    "     T.StructField(\"trip_distance\", T.FloatType()),\n",
    "     T.StructField(\"rate_code_id\", T.IntegerType()),\n",
    "     T.StructField(\"store_and_fwd_flag\", T.StringType()),\n",
    "     T.StructField(\"pu_location_id\", T.IntegerType()),\n",
    "     T.StructField(\"do_location_id\", T.IntegerType()),\n",
    "     T.StructField(\"payment_type\", T.StringType()),\n",
    "     T.StructField(\"fare_amount\", T.FloatType()),\n",
    "     T.StructField(\"extra\", T.FloatType()),\n",
    "     T.StructField(\"mta_tax\", T.FloatType()),\n",
    "     T.StructField(\"tip_amount\", T.FloatType()),\n",
    "     T.StructField(\"tolls_amount\", T.FloatType()),\n",
    "     T.StructField(\"improvement_surcharge\", T.FloatType()),\n",
    "     T.StructField(\"total_amount\", T.FloatType()),\n",
    "     T.StructField(\"congestion_surcharge\", T.FloatType()),\n",
    "     ])\n",
    "\n",
    "RIDE_SCHEMA = T.StructType(\n",
    " [T.StructField(\"vendor_id\", T.IntegerType()),\n",
    "  T.StructField('tpep_pickup_datetime', T.TimestampType()),\n",
    "  T.StructField('tpep_dropoff_datetime', T.TimestampType()),\n",
    "  T.StructField(\"passenger_count\", T.IntegerType()),\n",
    "  T.StructField(\"trip_distance\", T.FloatType()),\n",
    "  T.StructField(\"payment_type\", T.IntegerType()),\n",
    "  T.StructField(\"total_amount\", T.FloatType()),\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_kafka(consume_topic: str):\n",
    "    # Spark Streaming DataFrame, connect to Kafka topic served at host in bootrap.servers option\n",
    "    df_stream = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092,broker:29092\") \\\n",
    "        .option(\"subscribe\", consume_topic) \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .option(\"checkpointLocation\", \"checkpoint\") \\\n",
    "        .load()\n",
    "    return df_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,org.apache.spark:spark-avro_2.12:3.3.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark-Notebook\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_kafka(consume_topic: str):\n",
    "    # Spark Streaming DataFrame, connect to Kafka topic served at host in bootrap.servers option\n",
    "    df_stream = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092,broker:29092\") \\\n",
    "        .option(\"subscribe\", consume_topic) \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .option(\"checkpointLocation\", \"checkpoint\") \\\n",
    "        .load()\n",
    "    return df_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Queries with streaming sources must be executed with writeStream.start();\nkafka",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\github\\de-zoomcamp\\week_6_stream_processing\\local\\test.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/github/de-zoomcamp/week_6_stream_processing/local/test.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_stream \u001b[39m=\u001b[39m read_from_kafka(CONSUME_TOPIC_RIDES_CSV)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/github/de-zoomcamp/week_6_stream_processing/local/test.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_stream\u001b[39m.\u001b[39mshow(\u001b[39m5\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\LAP14062-local\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a bool\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    605\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[1;32m--> 606\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jdf\u001b[39m.\u001b[39mshowString(n, \u001b[39m20\u001b[39m, vertical))\n\u001b[0;32m    607\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    608\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\LAP14062-local\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\LAP14062-local\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Queries with streaming sources must be executed with writeStream.start();\nkafka"
     ]
    }
   ],
   "source": [
    "df_stream = read_from_kafka(CONSUME_TOPIC_RIDES_CSV)\n",
    "df_stream.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
